{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import copy\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_custom_image_name=\"ppi-extractor:gpu-1.0.0-201910130520\"\n",
    "instance_type = \"ml.p3.8xlarge\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(account_id, region, pytorch_custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Configure train/ test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_fullfile = \"s3://{}/aimed/AIMedFull.json\".format(bucket)\n",
    "plain_trainfile = \"s3://{}/aimed/AIMedtrain.json\".format(bucket)\n",
    "plain_valfile = \"s3://{}/aimed/AIMedval.json\".format(bucket)\n",
    "\n",
    "\n",
    "\n",
    "processed_fullfile = \"s3://{}/aimed/AIMedFull_preprocessed.json\".format(bucket)\n",
    "processed_trainfile = \"s3://{}/aimed/AIMedFull_preprocessed.json\".format(bucket)\n",
    "processed_valfile=\"s3://{}/aimed/AIMedval_preprocessed.json\".format(bucket)\n",
    "\n",
    "\n",
    "ylhsieh_fullfile=\"s3://{}/aimed/AIMedFull_Ylhsieh.json\".format(bucket)\n",
    "    \n",
    "\n",
    "embeddingfile=\"s3://{}/embeddings/bio_nlp_vec/PubMed-shuffle-win-2.bin.txt\".format(bucket)\n",
    "embed_dim=200\n",
    "\n",
    "#Pretrainedbert\n",
    "pretrained_bert=\"s3://{}/embeddings/bert/\".format(bucket)\n",
    "\n",
    "\n",
    "#Collobert embedding\n",
    "coll_embeddingfile=\"s3://{}/embeddings/collobert/words_vocab_collabert.txt\".format(bucket)\n",
    "coll_embed_dim=50\n",
    "\n",
    "\n",
    "pyyaslao_embeddingfile=\"s3://{}/embeddings/PubMed-and-PMC-w2v.bin.txt\".format(bucket)\n",
    "#embeddingfile=\"s3://{}/embeddings/bio_nlp_vec/PubMed-shuffle-win-30.bin.txt\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_path= \"s3://{}/results/\".format(bucket)\n",
    "s3_code_path= \"s3://{}/aimed_code\".format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_id = \"e582183f369e01418f651f5c664d52d1aeeac349\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docid = \"docid\"\n",
    "labelid=\"isValid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_plain_inputs = {\n",
    "    \"train\" : plain_fullfile,\n",
    "    \"PRETRAINED_BIOBERT\" : pretrained_bert\n",
    "}\n",
    "\n",
    "embedding_plain_inputs = {\n",
    "   \"train\" : plain_fullfile,\n",
    "   \"embedding\" : embeddingfile\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_processed_inputs = {\n",
    "    \"train\" : processed_fullfile,\n",
    "    \"PRETRAINED_BIOBERT\" : pretrained_bert\n",
    "\n",
    "}\n",
    "\n",
    "embedding_processed_inputs = {\n",
    "   \"train\" : processed_fullfile,\n",
    "   \"embedding\" : embeddingfile\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainval_plain_inputs = {\n",
    "    \"train\" : plain_trainfile,\n",
    "    \"val\" : plain_valfile,\n",
    "    \"PRETRAINED_BIOBERT\" : pretrained_bert\n",
    "\n",
    "}\n",
    "\n",
    "embedding_trainval_plain_inputs = {\n",
    "    \"train\" : plain_trainfile,\n",
    "    \"val\" : plain_valfile,\n",
    "   \"embedding\" : embeddingfile\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_trainval_processed_inputs = {\n",
    "    \"train\" : processed_trainfile,\n",
    "    \"val\" : processed_valfile,\n",
    "    \"PRETRAINED_BIOBERT\" : pretrained_bert\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "embedding_trainval_processed_inputs = {\n",
    "    \"train\" : processed_trainfile,\n",
    "    \"val\" : processed_valfile,\n",
    "    \"embedding\" : embeddingfile\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylhsieh_inputs = {\n",
    "    \"train\" : ylhsieh_fullfile,\n",
    "    \"embedding\" : embeddingfile\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_loss_objective_metric = 1\n",
    "loss_function_factory = \"algorithms.top_k_cross_entropy_loss_factory.TopKCrossEntropyLossFactory\"\n",
    "top_k_loss = 32\n",
    "batchsize = 32\n",
    "lr = .0001\n",
    "patience = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BILstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_full_plain_overlap_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetFactory\",\n",
    "    \"network\" :\"RelationExtractorBiLstmNetworkFactoryNoPos\",\n",
    "    \"trainfile\":plain_fullfile.split(\"/\")[-1],\n",
    "    \"embeddingfile\":embeddingfile.split(\"/\")[-1],\n",
    "    \"embeddim\":embed_dim,\n",
    "    \"batchsize\": batchsize,\n",
    "    \"epochs\" : \"1000\",  \n",
    "    \"earlystoppingpatience\":patience,\n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"learningrate\":lr,\n",
    "    \"lstm_dropout\":0.5,\n",
    "    \"pooling_kernel_size\":3,\n",
    "    \"lstm_num_layers\" :3,\n",
    "    \"lstm_hidden_size\":64,\n",
    "    \"fc_layer_size\":64,\n",
    "    \"fc_drop_out_rate\":0.5,\n",
    "    \"labelfieldname\":labelid,\n",
    "    \"commit_id\" : commit_id,\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "    \"top_k_loss\" :top_k_loss\n",
    "}\n",
    "inputs_bilstm_full_plain_overlap_hp={\n",
    "    \"inputs\":  embedding_plain_inputs,\n",
    "    \"hp\": bilstm_full_plain_overlap_hp,\n",
    "    \"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "bilstm_full_processed_overlap_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetPreprocessedFactory\",\n",
    "    \"network\" :\"RelationExtractorBiLstmNetworkFactoryNoPos\",\n",
    "    \"trainfile\":processed_fullfile.split(\"/\")[-1],\n",
    "    \"embeddingfile\":embeddingfile.split(\"/\")[-1],\n",
    "    \"embeddim\":embed_dim,\n",
    "    \"batchsize\": batchsize,\n",
    "    \"epochs\" : \"1000\",  \n",
    "    \"earlystoppingpatience\":patience,\n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"learningrate\":lr,\n",
    "    \"lstm_dropout\":0.5,\n",
    "    \"pooling_kernel_size\":3,\n",
    "    \"lstm_num_layers\" :3,\n",
    "    \"lstm_hidden_size\":64,\n",
    "    \"fc_layer_size\":64,\n",
    "    \"fc_drop_out_rate\":0.5,\n",
    "    \"labelfieldname\":labelid,\n",
    "    \"commit_id\" : commit_id,\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "     \"top_k_loss\" :top_k_loss\n",
    "}\n",
    "inputs_bilstm_full_processed_overlap_hp={\n",
    "    \"inputs\":  embedding_processed_inputs,\n",
    "    \"hp\": bilstm_full_processed_overlap_hp,\n",
    "    \"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Unique\n",
    "bilstm_full_plain_unique_hp = copy.deepcopy(bilstm_full_plain_overlap_hp)\n",
    "bilstm_full_plain_unique_hp[\"docidfieldname\"] = docid\n",
    "inputs_bilstm_full_plain_unique_hp= {\n",
    "    \"inputs\" :embedding_plain_inputs,\n",
    "    \"hp\": bilstm_full_plain_unique_hp,\n",
    "    \"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "bilstm_full_processed_unique_hp = copy.deepcopy(bilstm_full_processed_overlap_hp)\n",
    "bilstm_full_processed_unique_hp[\"docidfieldname\"] = docid\n",
    "inputs_bilstm_full_processed_unique_hp= {\n",
    "    \"inputs\" :embedding_processed_inputs ,\n",
    "    \"hp\": bilstm_full_processed_unique_hp,\n",
    "    \"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#split\n",
    "bilstm_trainval_processed_unique_hp = copy.deepcopy(bilstm_full_processed_overlap_hp)\n",
    "bilstm_trainval_processed_unique_hp[\"valfile\"] = processed_valfile.split(\"/\")[-1]\n",
    "bilstm_trainval_processed_unique_hp[\"trainfile\"] = processed_trainfile.split(\"/\")[-1]\n",
    "inputs_bilstm_trainval_processed_unique_hp= {\n",
    "    \"inputs\" :embedding_trainval_processed_inputs ,\n",
    "    \"hp\": bilstm_trainval_processed_unique_hp,\n",
    "    \"entry\":\"main_train.py\"\n",
    "}\n",
    "\n",
    "# Fill list\n",
    "bilstm_inputs_hps = {\"bilstm-full-processed-unique\" : inputs_bilstm_full_processed_unique_hp,\n",
    "           \"bilstm-full-plain-unique\":inputs_bilstm_full_plain_unique_hp,\n",
    "           \"bilstm-full-processed-overlap\" : inputs_bilstm_full_processed_overlap_hp,\n",
    "           \"bilstm-full-plain-overlap\":inputs_bilstm_full_plain_overlap_hp\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_ylsieh_overlap_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetFactoryYlhsieh\",\n",
    "    \"network\" :\"RelationExtractorBiLstmNetworkFactoryNoPos\",\n",
    "    \"trainfile\":ylhsieh_fullfile.split(\"/\")[-1],\n",
    "    \"embeddingfile\":embeddingfile.split(\"/\")[-1],\n",
    "    \"embeddim\":embed_dim,\n",
    "    \"batchsize\": batchsize,\n",
    "    \"epochs\" : \"1000\",  \n",
    "    \"earlystoppingpatience\":patience,\n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"learningrate\":lr,\n",
    "    \"lstm_dropout\":0.5,\n",
    "    \"lstm_num_layers\" :1,\n",
    "    \"lstm_hidden_size\":400,\n",
    "    \"fc_drop_out_rate\":0.5,\n",
    "    \"train_val_vocab_merge\":1,\n",
    "    \"docidfieldname\":None,\n",
    "    \"labelfieldname\":labelid,\n",
    "    \"commit_id\" : commit_id,\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "     \"top_k_loss\" :top_k_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_full_processed_overlap_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetPreprocessedFactory\",\n",
    "    \"network\" :\"RelationExtractorSimpleResnetCnnPosNetworkFactory\",\n",
    "    \"earlystoppingpatience\" : patience,\n",
    "    \"trainfile\":processed_fullfile.split(\"/\")[-1],\n",
    "    \"embeddingfile\":embeddingfile.split(\"/\")[-1],\n",
    "    \"embeddim\":embed_dim,\n",
    "    \"batchsize\": batchsize,\n",
    "    \"epochs\" : \"1000\",   \n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"dropout_rate_cnn\": 0.5,\n",
    "    \"pooling_kernel_size\": 3,\n",
    "    \"pool_stride\":1,\n",
    "    \"cnn_kernel_size\":3,\n",
    "    \"cnn_num_layers\":5,\n",
    "    \"cnn_output\":256,\n",
    "    \"learningrate\":lr,\n",
    "    \"weight_decay\":.00001,\n",
    "    \"fc_layer_size\": 512,\n",
    "    \"fc_drop_out_rate\": 0.5,\n",
    "    \"input_drop_out_rate\" : 0.2,\n",
    "    \"use_min_dict\":0,\n",
    "    \"train_val_vocab_merge\":0,\n",
    "    \"commit_id\":commit_id,    \n",
    "    \"labelfieldname\":labelid,\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "    \"top_k_loss\" :top_k_loss\n",
    "}\n",
    "\n",
    "inputs_resnet_full_processed_overlap_hp={\n",
    "    \"inputs\":  embedding_processed_inputs,\n",
    "    \"hp\": resnet_full_processed_overlap_hp,\n",
    "    \"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "resnet_full_plain_overlap_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetFactory\",\n",
    "    \"network\" :\"RelationExtractorSimpleResnetCnnPosNetworkFactory\",\n",
    "    \"earlystoppingpatience\" : patience,\n",
    "    \"trainfile\":plain_fullfile.split(\"/\")[-1],\n",
    "    \"embeddingfile\":embeddingfile.split(\"/\")[-1],\n",
    "    \"embeddim\":embed_dim,\n",
    "    \"batchsize\": batchsize,\n",
    "    \"epochs\" : \"1000\",   \n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"dropout_rate_cnn\": 0.5,\n",
    "    \"pooling_kernel_size\": 3,\n",
    "    \"pool_stride\":1,\n",
    "    \"cnn_kernel_size\":3,\n",
    "    \"cnn_num_layers\":5,\n",
    "    \"cnn_output\":256,\n",
    "    \"learningrate\":lr,\n",
    "    \"weight_decay\":.00001,\n",
    "    \"fc_layer_size\": 512,\n",
    "    \"fc_drop_out_rate\": 0.5,\n",
    "    \"input_drop_out_rate\" : 0.2,\n",
    "    \"use_min_dict\":0,\n",
    "    \"train_val_vocab_merge\":0,\n",
    "    \"commit_id\":commit_id,    \n",
    "    \"labelfieldname\":labelid,\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "    \"top_k_loss\" :top_k_loss\n",
    "}\n",
    "\n",
    "inputs_resnet_full_plain_overlap_hp={\n",
    "    \"inputs\":  embedding_plain_inputs,\n",
    "    \"hp\": resnet_full_plain_overlap_hp,\n",
    "    \"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#unique\n",
    "resnet_full_plain_unique_hp = copy.deepcopy(resnet_full_plain_overlap_hp)\n",
    "resnet_full_plain_unique_hp[\"docidfieldname\"] = docid\n",
    "inputs_resnet_full_plain_unique_hp = {\n",
    "\"inputs\" :embedding_plain_inputs,\n",
    "\"hp\":resnet_full_plain_unique_hp,\n",
    "\"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "resnet_full_processed_unique_hp = copy.deepcopy(resnet_full_processed_overlap_hp)\n",
    "resnet_full_processed_unique_hp[\"docidfieldname\"] = docid\n",
    "inputs_resnet_full_processed_unique_hp = {\n",
    "\"inputs\" :embedding_processed_inputs,\n",
    "\"hp\":resnet_full_processed_unique_hp,\n",
    "\"entry\":\"main_train_k_fold.py\"\n",
    "}\n",
    "\n",
    "\n",
    "#split\n",
    "resnet_trainval_processed_unique_hp = copy.deepcopy(resnet_full_processed_overlap_hp)\n",
    "resnet_trainval_processed_unique_hp[\"valfile\"] = processed_valfile.split(\"/\")[-1]\n",
    "resnet_trainval_processed_unique_hp[\"trainfile\"] = processed_trainfile.split(\"/\")[-1]\n",
    "inputs_resnet_trainval_processed_unique_hp= {\n",
    "    \"inputs\" :embedding_trainval_processed_inputs ,\n",
    "    \"hp\": resnet_trainval_processed_unique_hp,\n",
    "    \"entry\":\"main_train.py\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Fill list\n",
    "resnet_inputs_hps = {\"resnet-full-processed-unique\" : inputs_resnet_full_processed_unique_hp,\n",
    "           \"resnet-full-plain-unique\":inputs_resnet_full_plain_unique_hp,\n",
    "           \"resnet-full-processed-overlap\" : inputs_resnet_full_processed_overlap_hp,\n",
    "           \"resnet-full-plain-overlap\":inputs_resnet_full_plain_overlap_hp\n",
    "           }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 8\n",
    "\n",
    "bert_full_processed_unique_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetPreprocessedFactory\",\n",
    "    \"network\" :\"RelationExtractorBioBertFactory\",\n",
    "    \"trainfile\":processed_fullfile.split(\"/\")[-1],\n",
    "    \"batchsize\": batchsize/accumulation_steps,\n",
    "    \"accumulation_steps\" : accumulation_steps,\n",
    "    \"epochs\" : \"1000\",   \n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"learningrate\":.00001,\n",
    "    \"earlystoppingpatience\":20,\n",
    "    \"commit_id\" : commit_id,\n",
    "    \"docidfieldname\":\"docid\",\n",
    "    \"labelfieldname\":\"isValid\",\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "    \"top_k_loss\" :top_k_loss\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "inputs_bert_full_processed_unique_hp = {\n",
    "    \"inputs\" : bert_processed_inputs,\n",
    "    \"hp\" : bert_full_processed_unique_hp,\n",
    "    \"entry\":\"main_train_bert_k_fold.py\"\n",
    "}\n",
    "\n",
    "bert_full_plain_unique_hp = {\n",
    "    \"dataset\":\"PpiAimedDatasetFactory\",\n",
    "    \"network\" :\"RelationExtractorBioBertFactory\",\n",
    "    \"trainfile\":plain_fullfile.split(\"/\")[-1],\n",
    "    \"batchsize\": batchsize/accumulation_steps,\n",
    "    \"accumulation_steps\" : accumulation_steps,\n",
    "    \"epochs\" : \"1000\",   \n",
    "    \"log-level\" : \"INFO\",\n",
    "    \"learningrate\":.00001,\n",
    "    \"earlystoppingpatience\":20,\n",
    "    \"commit_id\" : commit_id,\n",
    "    \"docidfieldname\":\"docid\",\n",
    "    \"labelfieldname\":\"isValid\",\n",
    "    \"use_loss_objective_metric\": use_loss_objective_metric,\n",
    "    \"loss_func_factory_name\":loss_function_factory,\n",
    "    \"top_k_loss\" :top_k_loss\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "inputs_bert_full_plain_unique_hp = {\n",
    "    \"inputs\" : bert_plain_inputs,\n",
    "    \"hp\": bert_full_plain_unique_hp,\n",
    "    \"entry\":\"main_train_bert_k_fold.py\"\n",
    "}\n",
    "\n",
    "\n",
    "bert_full_processed_overlap_hp = copy.deepcopy( bert_full_processed_unique_hp)\n",
    "bert_full_processed_overlap_hp[\"docidfieldname\"] = None\n",
    "inputs_bert_full_processed_overlap_hp = copy.deepcopy( inputs_bert_full_processed_unique_hp)\n",
    "inputs_bert_full_processed_overlap_hp[\"hp\"] = bert_full_processed_overlap_hp\n",
    "\n",
    "\n",
    "\n",
    "bert_full_plain_overlap_hp = copy.deepcopy( bert_full_plain_unique_hp)\n",
    "bert_full_plain_overlap_hp[\"docidfieldname\"] = None\n",
    "inputs_bert_full_plain_overlap_hp = copy.deepcopy( inputs_bert_full_plain_unique_hp)\n",
    "inputs_bert_full_plain_overlap_hp[\"hp\"] = bert_full_plain_overlap_hp\n",
    "\n",
    "\n",
    "bert_inputs_hps = {\"bert-full-processed-unique\" : inputs_bert_full_processed_unique_hp,\n",
    "           \"bert-full-plain-unique\":inputs_bert_full_plain_unique_hp,\n",
    "           \"bert-full-processed-overlap\" : inputs_bert_full_processed_overlap_hp,\n",
    "           \"bert-full-plain-overlap\":inputs_bert_full_plain_overlap_hp\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hps = {}\n",
    "for k in bert_inputs_hps:\n",
    "    all_hps[k] = bert_inputs_hps[k]\n",
    "    \n",
    "for k in resnet_inputs_hps:\n",
    "    all_hps[k] = resnet_inputs_hps[k]\n",
    "    \n",
    "for k in bilstm_inputs_hps:\n",
    "    all_hps[k] = bilstm_inputs_hps[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainFScore\",\n",
    "                     \"Regex\": \"###score: train_fscore### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationFScore\",\n",
    "                     \"Regex\": \"###score: val_fscore### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit e582183f369e01418f651f5c664d52d1aeeac349\n",
      "    Add additional args\n"
     ]
    }
   ],
   "source": [
    "!git log -1 | head -1\n",
    "!git log -1 | head -5 | tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'----Jobname: aimed-resnet-full-processed-unique----'\n",
      "{   'batchsize': 32,\n",
      "    'cnn_kernel_size': 3,\n",
      "    'cnn_num_layers': 5,\n",
      "    'cnn_output': 256,\n",
      "    'commit_id': 'e582183f369e01418f651f5c664d52d1aeeac349',\n",
      "    'dataset': 'PpiAimedDatasetPreprocessedFactory',\n",
      "    'docidfieldname': 'docid',\n",
      "    'dropout_rate_cnn': 0.5,\n",
      "    'earlystoppingpatience': 50,\n",
      "    'embeddim': 200,\n",
      "    'embeddingfile': 'PubMed-shuffle-win-2.bin.txt',\n",
      "    'epochs': '1000',\n",
      "    'fc_drop_out_rate': 0.5,\n",
      "    'fc_layer_size': 512,\n",
      "    'input_drop_out_rate': 0.2,\n",
      "    'labelfieldname': 'isValid',\n",
      "    'learningrate': 0.0001,\n",
      "    'log-level': 'INFO',\n",
      "    'loss_func_factory_name': 'algorithms.top_k_cross_entropy_loss_factory.TopKCrossEntropyLossFactory',\n",
      "    'network': 'RelationExtractorSimpleResnetCnnPosNetworkFactory',\n",
      "    'pool_stride': 1,\n",
      "    'pooling_kernel_size': 3,\n",
      "    'top_k_loss': 32,\n",
      "    'train_val_vocab_merge': 0,\n",
      "    'trainfile': 'AIMedFull_preprocessed.json',\n",
      "    'use_loss_objective_metric': 1,\n",
      "    'use_min_dict': 0,\n",
      "    'weight_decay': 1e-05}\n",
      "{   'embedding': 's3://aegovan-data/embeddings/bio_nlp_vec/PubMed-shuffle-win-2.bin.txt',\n",
      "    'train': 's3://aegovan-data/aimed/AIMedFull_preprocessed.json'}\n",
      "'main_train_k_fold.py'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import pprint\n",
    "\n",
    "restrict_job_type = None    \n",
    "restrict_job_type= \"resnet-full-processed-unique\"\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for job_type in resnet_inputs_hps:\n",
    "    if restrict_job_type is not None and restrict_job_type != job_type: continue\n",
    "        \n",
    "    base_job_name=\"aimed-\" + job_type\n",
    "\n",
    "    hyperparameters = all_hps[job_type][\"hp\"]\n",
    "    inputs = all_hps[job_type][\"inputs\"]\n",
    "    entry_point= all_hps[job_type][\"entry\"]\n",
    "    \n",
    "    pp.pprint(\"----Jobname: {}----\".format(base_job_name))\n",
    "    pp.pprint(hyperparameters)\n",
    "    pp.pprint(inputs)\n",
    "    pp.pprint(entry_point)\n",
    "    \n",
    "    git_config = {'repo': 'https://github.com/elangovana/PPI-typed-relation-extractor.git',\n",
    "              'branch': 'master',\n",
    "              'commit': hyperparameters[\"commit_id\"]\n",
    "             }\n",
    "\n",
    "    \n",
    "\n",
    "    estimator = PyTorch(\n",
    "                   entry_point=entry_point,\n",
    "                    source_dir = 'source/algorithms',\n",
    "                    dependencies =['source/algorithms', 'source/datasets', 'source/preprocessor', 'source/modelnetworks', 'source/metrics'],\n",
    "                    role=role,\n",
    "                    framework_version =\"1.0.0\",\n",
    "                    py_version='py3',\n",
    "                    #git_config= git_config,\n",
    "                    image_name= docker_repo,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=instance_type,\n",
    "                    hyperparameters =hyperparameters,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    #train_use_spot_instances = True\n",
    "                    train_volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    train_max_run = 60 * 60 * 24 * 4,\n",
    "                    base_job_name = base_job_name)\n",
    "    \n",
    "    estimator.fit(inputs, wait=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
