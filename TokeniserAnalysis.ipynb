{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Tokeniser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./tests/temp/biobert\"\n",
    "bucket = \"aegovan-data\"\n",
    "trainfile = \"s3://{}/processed_dataset/train_multiclass.json\".format(bucket)\n",
    "testfile = \"s3://{}/processed_dataset/test_multiclass.json\".format(bucket)\n",
    "valfile = \"s3://{}/processed_dataset/val_multiclass.json\".format(bucket)\n",
    "\n",
    "\n",
    "\n",
    "column=\"normalised_abstract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"test_ensemble.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "\n",
    "tokeniser = BertTokenizer.from_pretrained(model_dir, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def download_single_file(bucket_name_path, local_path):\n",
    "    index = bucket_name_path.find(\"://\")\n",
    "\n",
    "    # remove the s3:// if present\n",
    "    if index > -1:\n",
    "        bucket_name_path = bucket_name_path[index + 3:]\n",
    "\n",
    "    key_start_index = bucket_name_path.find(\"/\")\n",
    "    bucket_name = bucket_name_path\n",
    "    key = \"/\"\n",
    "    if key_start_index > -1:\n",
    "        bucket_name = bucket_name_path[0:key_start_index]\n",
    "        key = bucket_name_path[key_start_index + 1:]\n",
    "        \n",
    "    client = boto3.resource('s3')\n",
    "    client.Bucket(bucket_name).download_file(key, local_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_counts(input_file, column):\n",
    "    data = pd.read_json(input_file)\n",
    "            \n",
    "    counts = []\n",
    "    \n",
    "    data[\"tokens\"] =  data.apply (lambda x:  tokeniser.tokenize(x[column]), axis=1)\n",
    "    data[\"token_len\"] = data.apply (lambda x:  len(x[\"tokens\"]), axis=1)\n",
    "    return data\n",
    "\n",
    "def get_counts_unique(input_file, column):\n",
    "    data = pd.read_json(input_file)\n",
    "            \n",
    "    pubmed_abstracts = pd.DataFrame( data[column].unique(), columns = [column])\n",
    "    \n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    result[\"tokens\"] =  pubmed_abstracts.apply (lambda x:  tokeniser.tokenize(x[column]), axis=1)\n",
    "    result[\"token_len\"] = result.apply (lambda x:  len(x[\"tokens\"]), axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_and_get_counts(s3_file):\n",
    "    local_file = os.path.join(\".\", s3_file.split(\"/\")[-1] )\n",
    "    download_single_file(s3_file, local_file) \n",
    "    df = get_counts (local_file, column)\n",
    "    df_unique = get_counts_unique (local_file, column)\n",
    "    \n",
    "    return df, df_unique\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2841.000000\n",
       "mean      379.406899\n",
       "std       101.442386\n",
       "min       142.000000\n",
       "0%        142.000000\n",
       "10%       246.000000\n",
       "20%       281.000000\n",
       "30%       314.000000\n",
       "40%       348.000000\n",
       "50%       380.000000\n",
       "60%       415.000000\n",
       "70%       446.000000\n",
       "80%       477.000000\n",
       "90%       514.000000\n",
       "max       740.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_train_unique  = download_and_get_counts(trainfile)\n",
    "\n",
    "df_train[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    358.000000\n",
       "mean     351.662011\n",
       "std      101.722808\n",
       "min      142.000000\n",
       "0%       142.000000\n",
       "10%      228.000000\n",
       "20%      257.400000\n",
       "30%      283.000000\n",
       "40%      314.800000\n",
       "50%      342.500000\n",
       "60%      371.200000\n",
       "70%      411.000000\n",
       "80%      438.000000\n",
       "90%      487.300000\n",
       "max      740.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     90.000000\n",
       "mean     327.222222\n",
       "std       95.787426\n",
       "min      127.000000\n",
       "0%       127.000000\n",
       "10%      211.800000\n",
       "20%      247.800000\n",
       "30%      279.200000\n",
       "40%      297.600000\n",
       "50%      323.000000\n",
       "60%      339.800000\n",
       "70%      369.000000\n",
       "80%      398.400000\n",
       "90%      439.900000\n",
       "max      618.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test, df_test_unique  = download_and_get_counts(testfile)\n",
    "df_test_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    720.000000\n",
       "mean     362.105556\n",
       "std       96.966708\n",
       "min      127.000000\n",
       "0%       127.000000\n",
       "10%      243.000000\n",
       "20%      282.000000\n",
       "30%      317.000000\n",
       "40%      333.000000\n",
       "50%      363.000000\n",
       "60%      375.000000\n",
       "70%      400.000000\n",
       "80%      431.000000\n",
       "90%      486.100000\n",
       "max      618.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     50.000000\n",
       "mean     317.340000\n",
       "std      105.959486\n",
       "min       57.000000\n",
       "0%        57.000000\n",
       "10%      213.700000\n",
       "20%      229.600000\n",
       "30%      259.400000\n",
       "40%      288.400000\n",
       "50%      311.500000\n",
       "60%      332.800000\n",
       "70%      347.600000\n",
       "80%      374.000000\n",
       "90%      479.800000\n",
       "max      586.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val, df_val_unique  = download_and_get_counts(valfile)\n",
    "df_val_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    366.000000\n",
       "mean     340.803279\n",
       "std       95.829898\n",
       "min       57.000000\n",
       "0%        57.000000\n",
       "10%      226.000000\n",
       "20%      258.000000\n",
       "30%      294.000000\n",
       "40%      315.000000\n",
       "50%      343.000000\n",
       "60%      344.000000\n",
       "70%      353.000000\n",
       "80%      378.000000\n",
       "90%      487.000000\n",
       "max      586.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"train_unique\"] = df_train_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])\n",
    "df[\"test_unique\"] = df_test_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])\n",
    "df[\"val_unique\"] = df_val_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  train\\_unique &  test\\_unique &  val\\_unique \\\\\n",
      "\\midrule\n",
      "count &        358.00 &        90.00 &       50.00 \\\\\n",
      "mean  &        351.66 &       327.22 &      317.34 \\\\\n",
      "std   &        101.72 &        95.79 &      105.96 \\\\\n",
      "min   &        142.00 &       127.00 &       57.00 \\\\\n",
      "0\\%    &        142.00 &       127.00 &       57.00 \\\\\n",
      "10\\%   &        228.00 &       211.80 &      213.70 \\\\\n",
      "20\\%   &        257.40 &       247.80 &      229.60 \\\\\n",
      "30\\%   &        283.00 &       279.20 &      259.40 \\\\\n",
      "40\\%   &        314.80 &       297.60 &      288.40 \\\\\n",
      "50\\%   &        342.50 &       323.00 &      311.50 \\\\\n",
      "60\\%   &        371.20 &       339.80 &      332.80 \\\\\n",
      "70\\%   &        411.00 &       369.00 &      347.60 \\\\\n",
      "80\\%   &        438.00 &       398.40 &      374.00 \\\\\n",
      "90\\%   &        487.30 &       439.90 &      479.80 \\\\\n",
      "max   &        740.00 &       618.00 &      586.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Analysis on Seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score,precision_score, recall_score\n",
    "\n",
    "def get_scores(df, actual, predicted, labels=None):\n",
    "\n",
    "    f1 = f1_score(df[actual], df[predicted], average='macro' ,sample_weight=None, labels=labels)  \n",
    "    p = precision_score(df[actual], df[predicted], average='macro' ,sample_weight=None)  \n",
    "    r = recall_score(df[actual], df[ predicted], average='macro' ,sample_weight=None)  \n",
    "\n",
    "    return f1,p,r\n",
    "\n",
    "def plot_confusion_matrix(df, actual, predicted, save_file=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scikitplot as skplt\n",
    "\n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(df[actual], df[predicted], normalize=True , figsize=(4,4),  x_tick_rotation=90)\n",
    "  \n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(df[actual], df[predicted], figsize=(4,4), normalize=False,x_tick_rotation=90 )\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = list(set(df[\"actual\"].unique().tolist()) - set(\"other\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len > 510\"), \"actual\" , \"ensemble_predicted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df.query(\"token_len > 510\"), \"actual\" , \"ensemble_predicted\", \"len_long_confusion.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len > 510\"), \"actual\" , \"ensemble_predicted\", positive_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df.query(\"token_len < 510\"), \"actual\" , \"ensemble_predicted\", \"len_short_confusion.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len > 510\").groupby( [\"actual\",\"ensemble_predicted\" ])[\"actual\",\"ensemble_predicted\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len >= 510\")[\"normalised_abstract\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len < 510\")[\"normalised_abstract\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len >= 510\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len < 510\"), \"actual\" , \"ensemble_predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len < 510\"), \"actual\" , \"ensemble_predicted\", positive_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df, \"actual\" , \"ensemble_predicted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
