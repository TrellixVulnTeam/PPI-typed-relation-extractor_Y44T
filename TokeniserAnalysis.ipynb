{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Tokeniser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./tests/temp/biobert\"\n",
    "bucket = \"aegovan-data\"\n",
    "trainfile = \"s3://{}/processed_dataset/train_multiclass.json\".format(bucket)\n",
    "testfile = \"s3://{}/processed_dataset/test_multiclass.json\".format(bucket)\n",
    "valfile = \"s3://{}/processed_dataset/val_multiclass.json\".format(bucket)\n",
    "\n",
    "\n",
    "\n",
    "column=\"normalised_abstract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"test_ensemble.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "\n",
    "tokeniser = BertTokenizer.from_pretrained(model_dir, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def download_single_file(bucket_name_path, local_path):\n",
    "    index = bucket_name_path.find(\"://\")\n",
    "\n",
    "    # remove the s3:// if present\n",
    "    if index > -1:\n",
    "        bucket_name_path = bucket_name_path[index + 3:]\n",
    "\n",
    "    key_start_index = bucket_name_path.find(\"/\")\n",
    "    bucket_name = bucket_name_path\n",
    "    key = \"/\"\n",
    "    if key_start_index > -1:\n",
    "        bucket_name = bucket_name_path[0:key_start_index]\n",
    "        key = bucket_name_path[key_start_index + 1:]\n",
    "        \n",
    "    client = boto3.resource('s3')\n",
    "    client.Bucket(bucket_name).download_file(key, local_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_counts(input_file, column):\n",
    "    data = pd.read_json(input_file)\n",
    "            \n",
    "    counts = []\n",
    "    \n",
    "    data[\"tokens\"] =  data.apply (lambda x:  tokeniser.tokenize(x[column]), axis=1)\n",
    "    data[\"token_len\"] = data.apply (lambda x:  len(x[\"tokens\"]), axis=1)\n",
    "    return data\n",
    "\n",
    "def get_counts_unique(input_file, column):\n",
    "    data = pd.read_json(input_file)\n",
    "            \n",
    "    pubmed_abstracts = pd.DataFrame( data[column].unique(), columns = [column])\n",
    "    \n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    result[\"tokens\"] =  pubmed_abstracts.apply (lambda x:  tokeniser.tokenize(x[column]), axis=1)\n",
    "    result[\"token_len\"] = result.apply (lambda x:  len(x[\"tokens\"]), axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_and_get_counts(s3_file):\n",
    "    local_file = os.path.join(\".\", s3_file.split(\"/\")[-1] )\n",
    "    download_single_file(s3_file, local_file) \n",
    "    df = get_counts (local_file, column)\n",
    "    df_unique = get_counts_unique (local_file, column)\n",
    "    \n",
    "    return df, df_unique\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3223.000000\n",
       "mean      375.686007\n",
       "std       102.796268\n",
       "min        57.000000\n",
       "0%         57.000000\n",
       "10%       253.000000\n",
       "20%       281.000000\n",
       "30%       311.000000\n",
       "40%       339.000000\n",
       "50%       368.000000\n",
       "60%       395.000000\n",
       "70%       430.000000\n",
       "80%       475.000000\n",
       "90%       513.000000\n",
       "max       795.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_train_unique  = download_and_get_counts(trainfile)\n",
    "\n",
    "df_train[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    459.000000\n",
       "mean     348.372549\n",
       "std      105.209868\n",
       "min       57.000000\n",
       "0%        57.000000\n",
       "10%      228.000000\n",
       "20%      258.000000\n",
       "30%      280.400000\n",
       "40%      311.000000\n",
       "50%      333.000000\n",
       "60%      362.000000\n",
       "70%      396.200000\n",
       "80%      432.000000\n",
       "90%      488.200000\n",
       "max      795.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    116.000000\n",
       "mean     338.974138\n",
       "std      103.532432\n",
       "min      175.000000\n",
       "0%       175.000000\n",
       "10%      223.000000\n",
       "20%      247.000000\n",
       "30%      275.000000\n",
       "40%      297.000000\n",
       "50%      317.500000\n",
       "60%      344.000000\n",
       "70%      381.500000\n",
       "80%      417.000000\n",
       "90%      513.000000\n",
       "max      618.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test, df_test_unique  = download_and_get_counts(testfile)\n",
    "df_test_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    894.000000\n",
       "mean     378.592841\n",
       "std      110.356765\n",
       "min      175.000000\n",
       "0%       175.000000\n",
       "10%      236.000000\n",
       "20%      283.600000\n",
       "30%      302.000000\n",
       "40%      338.000000\n",
       "50%      354.000000\n",
       "60%      392.000000\n",
       "70%      431.000000\n",
       "80%      520.000000\n",
       "90%      533.000000\n",
       "max      618.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     65.000000\n",
       "mean     349.815385\n",
       "std      102.990426\n",
       "min      142.000000\n",
       "0%       142.000000\n",
       "10%      215.000000\n",
       "20%      254.400000\n",
       "30%      298.800000\n",
       "40%      320.200000\n",
       "50%      347.000000\n",
       "60%      362.400000\n",
       "70%      402.800000\n",
       "80%      438.400000\n",
       "90%      478.400000\n",
       "max      586.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val, df_val_unique  = download_and_get_counts(valfile)\n",
    "df_val_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    420.000000\n",
       "mean     357.330952\n",
       "std       90.513505\n",
       "min      142.000000\n",
       "0%       142.000000\n",
       "10%      237.000000\n",
       "20%      284.000000\n",
       "30%      315.000000\n",
       "40%      330.000000\n",
       "50%      358.000000\n",
       "60%      363.000000\n",
       "70%      398.000000\n",
       "80%      438.000000\n",
       "90%      464.000000\n",
       "max      586.000000\n",
       "Name: token_len, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"train_unique\"] = df_train_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])\n",
    "df[\"test_unique\"] = df_test_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])\n",
    "df[\"val_unique\"] = df_val_unique[\"token_len\"].describe(percentiles=[i/100 for i in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "{} &  train\\_unique &  test\\_unique &  val\\_unique \\\\\n",
      "\\midrule\n",
      "count &        459.00 &       116.00 &       65.00 \\\\\n",
      "mean  &        348.37 &       338.97 &      349.82 \\\\\n",
      "std   &        105.21 &       103.53 &      102.99 \\\\\n",
      "min   &         57.00 &       175.00 &      142.00 \\\\\n",
      "0\\%    &         57.00 &       175.00 &      142.00 \\\\\n",
      "10\\%   &        228.00 &       223.00 &      215.00 \\\\\n",
      "20\\%   &        258.00 &       247.00 &      254.40 \\\\\n",
      "30\\%   &        280.40 &       275.00 &      298.80 \\\\\n",
      "40\\%   &        311.00 &       297.00 &      320.20 \\\\\n",
      "50\\%   &        333.00 &       317.50 &      347.00 \\\\\n",
      "60\\%   &        362.00 &       344.00 &      362.40 \\\\\n",
      "70\\%   &        396.20 &       381.50 &      402.80 \\\\\n",
      "80\\%   &        432.00 &       417.00 &      438.40 \\\\\n",
      "90\\%   &        488.20 &       513.00 &      478.40 \\\\\n",
      "max   &        795.00 &       618.00 &      586.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.round(2).to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Analysis on Seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score,precision_score, recall_score\n",
    "\n",
    "def get_scores(df, actual, predicted, labels=None):\n",
    "\n",
    "    f1 = f1_score(df[actual], df[predicted], average='macro' ,sample_weight=None, labels=labels)  \n",
    "    p = precision_score(df[actual], df[predicted], average='macro' ,sample_weight=None)  \n",
    "    r = recall_score(df[actual], df[ predicted], average='macro' ,sample_weight=None)  \n",
    "\n",
    "    return f1,p,r\n",
    "\n",
    "def plot_confusion_matrix(df, actual, predicted, save_file=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scikitplot as skplt\n",
    "\n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(df[actual], df[predicted], normalize=True , figsize=(4,4),  x_tick_rotation=90)\n",
    "  \n",
    "\n",
    "    skplt.metrics.plot_confusion_matrix(df[actual], df[predicted], figsize=(4,4), normalize=False,x_tick_rotation=90 )\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = list(set(df[\"actual\"].unique().tolist()) - set(\"other\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len > 510\"), \"actual\" , \"ensemble_predicted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df.query(\"token_len > 510\"), \"actual\" , \"ensemble_predicted\", \"len_long_confusion.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len > 510\"), \"actual\" , \"ensemble_predicted\", positive_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df.query(\"token_len < 510\"), \"actual\" , \"ensemble_predicted\", \"len_short_confusion.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len > 510\").groupby( [\"actual\",\"ensemble_predicted\" ])[\"actual\",\"ensemble_predicted\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len >= 510\")[\"normalised_abstract\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len < 510\")[\"normalised_abstract\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"token_len >= 510\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len < 510\"), \"actual\" , \"ensemble_predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df.query(\"token_len < 510\"), \"actual\" , \"ensemble_predicted\", positive_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores ( df, \"actual\" , \"ensemble_predicted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
